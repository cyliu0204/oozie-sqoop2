#sqoop 地址
Sqoop2Url=http://12.2.4.131:12000/sqoop/

##jdbc配置
oozie.sqoop2.link.LinkCreator.JdbcUrl=jdbc:mysql://12.2.4.106:3306/hue
oozie.sqoop2.link.LinkCreator.JdbcDriver=com.mysql.jdbc.Driver
oozie.sqoop2.link.LinkCreator.JdbcUser=root
oozie.sqoop2.link.LinkCreator.JdbcPasswd=iflytek
#jdbc配置完毕

#hdfs配置
oozie.sqoop2.link.LinkCreator.HdfsUrl=hdfs://12.2.4.131:8020
#hdfs配置完毕


#kafka配置
oozie.sqoop2.link.LinkCreator.BrokerList=12.2.4.133:9092,12.2.4.134:9092,12.2.4.135:9092
oozie.sqoop2.link.LinkCreator.ZookeeperConnect=12.2.4.133:2181,12.2.4.134:2181,12.2.4.135:2181
#kakfa配置

#hive配置
 oozie.sqoop2.link.LinkCreator.HiveUrl= jdbc:hive2://12.2.4.131:10000/test
 oozie.sqoop2.link.LinkCreator.HdfsUri=hdfs://12.2.4.107:8020
 oozie.sqoop2.link.LinkCreator.HiveUser=root
 oozie.sqoop2.link.LinkCreator.HivePasswd=iflytek
#hive配置结束

#如果 创建link,connector需要制定，可选项：hdfs-connector、generic-jdbc-connector、kafka-connector、kite-connector,hive-connector
#是否创建link 必填
oozie.sqoop2.action.Sqoop2Action.IsCreateFromLink=0
#创建link
oozie.sqoop2.action.Sqoop2Action.FromConnectorName=hdfs-connector
oozie.sqoop2.action.Sqoop2Action.FromLinkName=hdfs_cyliu

#如果 创建link,connector需要制定，可选项：hdfs-connector、generic-jdbc-connector、kafka-connector、kite-connector
#是否创建link 必填
oozie.sqoop2.action.Sqoop2Action.IsCreateToLink=0
#创建link
oozie.sqoop2.action.Sqoop2Action.ToConnectorName=hive-connector
oozie.sqoop2.action.Sqoop2Action.ToLinkName=hive




oozie.sqoop2.job.JobAction.JobName=mysql_hive

#创建fromjob
# jdbc  
oozie.sqoop2.job.JobAction.jdbc.FromSchemaName=test
oozie.sqoop2.job.JobAction.jdbc.FromTableName=student
oozie.sqoop2.job.JobAction.jdbc.FromPartitionColumn=id

#标识增量的列
oozie.sqoop2.job.JobAction.jdbc.FromCheckColumn=id
#初始值   
oozie.sqoop2.job.JobAction.jdbc.FromCheckValue=0
#jdbc结束

#hdfs  
oozie.sqoop2.job.JobAction.hdfs.FromHdfsUrl=/home/6a/2017-07-07/student1
#hdfs結束

#hive start
oozie.sqoop2.job.JobAction.hive.FromTableName=student1
oozie.sqoop2.job.JobAction.hive.InputDir=/home/3a
oozie.sqoop2.job.JobAction.hive.FromNullValue=
oozie.sqoop2.job.JobAction.hive.FromOverrideNullValue=
#是否增量 0 NONE 1：NEW FILES
oozie.sqoop2.job.JobAction.hive.IncrementalType=0

#创建fromjob结束



#创建tojob

# jdbc 必填
oozie.sqoop2.job.JobAction.jdbc.ToSchemaName=
oozie.sqoop2.job.JobAction.jdbc.ToTableName=
oozie.sqoop2.job.JobAction.jdbc.ToPartitionColumn=
#jdbc结束

#hdfs 必填
oozie.sqoop2.job.JobAction.hdfs.ToHdfsUrl=/usr/temp/
#File format: 
#  0 : TEXT_FILE
#  1 : SEQUENCE_FILE
#  2 : PARQUET_FILE
oozie.sqoop2.job.JobAction.hdfs.TOHdfsFileFormat=NONE
#Compression codec: 
#  0 : NONE
#  1 : DEFAULT
#  2 : DEFLATE
#  3 : GZIP
#  4 : BZIP2
#  5 : LZO
#  6 : LZ4
#  7 : SNAPPY
#  8 : CUSTOM
oozie.sqoop2.job.JobAction.hdfs.TOCompressionCodec=NONE
#hdfs 结束

#kafka必填
oozie.sqoop2.job.JobAction.kafka.ToTopic=test-redcao


#hive start
oozie.sqoop2.job.JobAction.hive.ToTableName=student1
oozie.sqoop2.job.JobAction.hive.OutputDir=/home/6a
oozie.sqoop2.job.JobAction.hive.ToNullValue=
oozie.sqoop2.job.JobAction.hive.ToOverrideNullValue=
#File format: 
#  0 : TEXT_FILE
#  1 : SEQUENCE_FILE
#  2 : PARQUET_FILE
oozie.sqoop2.job.JobAction.hive.TOHdfsFileFormat=PARQUET_FILE
#Compression codec: 
#  0 : NONE
#  1 : DEFAULT
#  2 : DEFLATE
#  3 : GZIP
#  4 : BZIP2
#  5 : LZO
#  6 : LZ4
#  7 : SNAPPY
#  8 : CUSTOM
oozie.sqoop2.job.JobAction.hive.TOCompressionCodec=NONE

#增量模式
oozie.sqoop2.job.JobAction.hive.ToAppendMode=true
#hive end
oozie.sqoop2.job.JobAction.NumExtractors=3
#创建tojob结束